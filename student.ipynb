{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Pedro Jofre Lora\n",
    "* Student pace: self paced\n",
    "* Scheduled project review date/time: 2/14/2019 10:00AM EST\n",
    "* Instructor name: Eli Thomas\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling King County Housing Prices Using MLR Supported with Cross-Validated RFE.\n",
    "### Table of Contents\n",
    "1. [Introduction](#1) <br>\n",
    "2. [Importing and Cleaning Data](#2) <br>\n",
    "3. [Exploring and Modifying Data](#3) <br>\n",
    "4. [Modeling Data](#4) <br>\n",
    "5. [Interpreting Data](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Introduction\n",
    "This is the introduction where you explain the relevant information given the question that was asked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Importing and cleaning data (Obtain and Scrub)\n",
    "### 2.1 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Import the data\n",
    "kc = pd.read_csv('kc_house_data.csv', index_col = 'id')\n",
    "kc.columns = [col_name.title() for col_name in list(kc.columns)]\n",
    "kc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data = kc.nunique().sort_values(), \n",
    "    columns = ['Counts']\n",
    "    ).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define problems\n",
    "1.  Date, View, and Sqft_basement are incorrect types.\n",
    "2.  Waterfront and zipcode likely need to be converted to categorical.\n",
    "3.  Condition and Grade might need to be converted to categorical, unless the numbers are a monotonic scale (scale of 1-5 with 5 being the \"best\").\n",
    "4.  Investigate Yr_Renovated.\n",
    "5.  Deciding what to do with the date.\n",
    "6.  Deciding what to do with the lat and long values.\n",
    "\n",
    "Data could be dropped from View, and it should also be converted to a binary category since the description is \"has been viewed\", and not \"the number of times the propery has been viewed\".\n",
    "\n",
    "Waterfront and zipcode may not need to be converted to categorical and could be tossed out if the distributions of the groups is equal. If the distributions are not equal then they'll both be changed to categorical. It might be best to run separate analyses for the categories if there's enough data, since the relationships may change in shape given the value of a category.\n",
    "\n",
    "Condition and Grade likely will not need to be converted to categorical, though we must assume that they are monotonic scales.\n",
    "\n",
    "There is a surprisingly low mean and a very high std for Yr_Renovated, which may indicate that the data may not accurately represent what's going on. It may be the case that very few homes have renovations on file. If this is the case, then Yr_renovated should be changed to Renovated, which is a binary category indicating whether or not a renovation is on file.\n",
    "\n",
    "The date will likely have a large impact on predicting the final sale price. I'll investigate to see if there is an impact, and then decide what to do.\n",
    "\n",
    "Changing Lat and Long data into rectangular sectors can approximate neighborhoods that can then be used as categories. It would be best to look at a map of king county to see if there are distinct neighborhoods that could be drawn instead. This would be an interesting function...\n",
    "\n",
    "\n",
    "### 2.3 Resolving problems\n",
    "#### 2.3.1. Change Date and Sqft_Basement to correct types. Drop Data as necessary. Make a separate Basement as a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.Sqft_Basement.replace(to_replace = '?', \n",
    "                         value = np.nan, \n",
    "                         inplace = True)\n",
    "kc.Sqft_Basement = kc.Sqft_Basement.apply(pd.to_numeric)\n",
    "kc = kc[kc.Sqft_Basement.notnull()]\n",
    "kc = kc[kc.View.notnull()]\n",
    "kc.View = kc.View.where(kc.View <= 0, other = 1)\n",
    "kc['View'] = (kc.View > 0).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc['Basement'] = (kc.Sqft_Basement>0).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.Date = pd.to_datetime(kc.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Change Waterfront and zipcode to categories if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfront_grouping = kc.groupby(by = 'Waterfront')\n",
    "waterfront_grouping.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style = 'dark')\n",
    "ax1 = waterfront_grouping.get_group(0).Price.plot.hist(density = True)\n",
    "waterfront_grouping.get_group(1).Price.plot.hist(density = True,\n",
    "                                            secondary_y = True,\n",
    "                                            alpha = 0.3,\n",
    "                                            ax = ax1)\n",
    "ax1.set_xlabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are clearly different in shape, so waterfront will be converted to a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.Waterfront.fillna(0.0,inplace = True) # Assume that a null listing means the house is not waterfront\n",
    "kc.Waterfront = (kc.Waterfront>0).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.groupby('Zipcode').Price.median().plot.box(figsize = (3,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medians of the groups are too spread out to believe that zipcode might not have a significant impact on the final price of the house. It will be converted to a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.Zipcode = kc.Zipcode.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Changing Grade and Condition to categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (12,4))\n",
    "kc.groupby('Grade').Price.median().plot(ax = ax1, title = 'Price by Grade');\n",
    "kc.groupby('Condition').Price.median().plot(ax = ax2, title = 'Price by Condition')\n",
    "ax1.set_ylabel('Price')\n",
    "ax2.set_ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price as a function of Condition is not easily approximated by a simple function, though this is odd. I'll investigate price by condition more closely to see if there's a reason behind the deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.groupby('Condition').Price.agg(['count','max','min','mean','median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min and max follow a monotonically increasing trend. The scale is most likely meant to be monotonic, so I'll treat it as such and leave Condition as a numerical feature. \n",
    "\n",
    "It is also strange that the lowest value for Grade has a higher price than the two following values for grade. I'll also investigate that further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.groupby('Grade').Price.agg(['count','max','min','mean','median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one record for a house with a grade of 3. This record will be eliminated since we don't know if this house is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop(kc.loc[kc.Grade == 3].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4  Changing Yr_Renovated to Yrs_Since_Renovation.\n",
    "This is relatively straightforward. The complicated part is that many of the homes have a value of 0.0 for the year renovated, which is nonsensical. It would be good to investigate how many homes have a renovation date on record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_unknown_rennovations = kc.Yr_Renovated.replace(to_replace = 0,\n",
    "                                                   value = np.nan).isna().sum()\n",
    "num_renovated = (len(kc) - num_unknown_rennovations)\n",
    "'The number of houses with documented renovations is {0:G}'.format(num_renovated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so few homes and so many features, I don't think it makes sense to treat this as a continuous variable for those homes that have been renovated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.Yr_Renovated.fillna(0, inplace = True)\n",
    "kc['Renovated'] = kc.Yr_Renovated.where(kc.Yr_Renovated <= 0, other = 1)\n",
    "kc['Renovated'] = (kc.Renovated > 0).astype('category')\n",
    "kc.drop(labels = 'Yr_Renovated', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Dealing with the date.\n",
    "\n",
    "My inclination here is that prices are higher in the summer months because demand is higher. I'll look at a 30 day rolling window to spot trends in the median price, and also resample the data by month to also look for trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,4))\n",
    "kc.groupby('Date').Price.median().rolling(30).median().plot(ax = ax1, title = 'Rolling Window')\n",
    "kc.reset_index().set_index(['Date']).resample('m').Price.median().plot(ax = ax2, title = 'Resampled Data')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the maximum 'swing' in price as compared to the median\n",
    "price_adjustment = (kc.reset_index().set_index(['Date']).resample('m').Price.median() / kc.Price.median() - 1)*100\n",
    "max_swing = price_adjustment.max()-price_adjustment.min()\n",
    "'The maximum swing in adjusted price is {0:2.2f}%'.format(max_swing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum swing on the median price is a hefty 11.61%! The model will have to account for the time of the year in order to be more accurate. The easiest way to do this will be to resample by month and then turn into a category. Complexity can be added to the model later by fitting a function to the date, instead of simply categorizing by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc['Month'] = kc.Date.dt.month_name().astype('category')\n",
    "kc.drop('Date', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Deciding what to do with lat and long values\n",
    "\n",
    "Here's a map of King County with zip-code boundaries:\n",
    "<img src=\"https://www.kingcounty.gov/~/media/operations/GIS/maps/vmc/images/zipcodes_westKC_586.ashx?la=en\" alt=\"An image of King County with zip-code boundaries\" title=\"King Country Zip Codes\" />\n",
    "<br><br><br>\n",
    "And, for comparison's sake, here's a map of Manhattan with zip-code boundaries:\n",
    "<img src=\"https://www.propertyshark.com/Real-Estate-Reports/wp-content/uploads/2012/10/infographic-zip-codes-3.png\" alt=\"An image of Manhattan with zip-code boundaries\" title=\"Manhattan Zip Codes\" />\n",
    "\n",
    "Zip code boundaries trace a mixture of natural boundaries and, I suspect, socioeconomic boundaries. This is corroborated by the Manhattan zipcodes, where neighborhoods are roughly approximated by zip codes (I live in NYC, so I'm more familiar with the neighborhoods here). Though zip codes are not a perfect stand-in for neighborhoods, they are adequate for a first pass at this model. If I have time, I will attend to making discrete quadrants that better capture pricing at the neighborhood level. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop(labels = ['Lat', 'Long'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up importing the data, I'll check the metadata again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks significantly better now. Some series' types could be changed to improve efficiency, but I'm not worried about it at this point since the dataframe is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Exploring and Modifying Data\n",
    "\n",
    "First I'll look at violin plots of Price for the predictors that have categorical values. Then I'll look at distributions for continuous predictors. Before I go much further, I'll look for correlations between the continuous predictors and make some decisions about what data to drop. Finally, I'll look at scatter plots of Price vs continous predictors. In order to make this less repetitive, I'll use ipywidgets to create interactive displays. I'll dive deeper as needed along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(kc.select_dtypes('category').columns)\n",
    "feature_list.remove('Zipcode')\n",
    "hue_list = feature_list.copy()\n",
    "hue_list.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def violin_plot(x=feature_list, \n",
    "                yscale = ['log', 'linear'],\n",
    "                hue = hue_list):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    if (hue != 'None') & (hue != x):\n",
    "        title = \"Price vs. \" + x + \" sorted by \" + hue\n",
    "        hue = kc[hue]\n",
    "    else:\n",
    "        hue = None\n",
    "        title = \"Price vs. \" + x\n",
    "    \n",
    "    \n",
    "    sns.violinplot(x = kc[x], \n",
    "                   y = kc.Price, \n",
    "                   cut = 0,\n",
    "                   scale = 'area',\n",
    "                   inner = 'box',\n",
    "                   hue = hue,\n",
    "                   ax = ax,\n",
    "                   )\n",
    "    plt.yscale(yscale)\n",
    "    plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Histograms of numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the histograms I noticed that there's a house with 33 bedrooms whose price and living area do not corroborate the number of bedrooms. I'll drop it from the dataframe since I can't be certain that it's supposed to have 3 bedrooms instead. That being said, I want to investigate the descriptive values again to make sure that I haven't missed any other obvious outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else looks okay. I'll get rid of the house with 33 bedrooms and then check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop(labels = kc.loc[kc.Bedrooms == 33].index, inplace = True)\n",
    "kc.Bedrooms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def hist_plot(data = list(kc.select_dtypes(include = [np.number]).columns),\n",
    "             log_data = [False, True]):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    title = \"Distribution of \" + data\n",
    "    \n",
    "    if log_data:\n",
    "        data = kc[data].replace(0,0.1)\n",
    "        data = data.apply(np.log10)\n",
    "    else:\n",
    "        data = kc[data]\n",
    "    sns.distplot(data,\n",
    "                 ax = ax)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictors = kc.drop(labels = 'Price', axis = 1)\n",
    "sns.set_style('white')\n",
    "corr = predictors.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.5, vmin = -0.5, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)\n",
    "plt.show()\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix shows that many of these variables correlate with each other. I suppose I'm not surprised by this. After all, a more expensive house tends to have more bedrooms, bathrooms, space, and floors. Additionally, it is likely to have a higher grade and be newer. So, those things tend to go together and intuitively they will be correlated.\n",
    "\n",
    "From the description of the columns, I don't think that we need to keep all three of: Sqft_living, Sqft_Above, and Sqft_Basement. Sqft_Above indicates the area of the house above the basement. It is likely that the sum of the area above the basement and the basement area is almost always the \"living area\". I'll check this assumption quickly. If this mostly holds true, then I'll drop \"living area\", if not then I'll drop \"above\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((kc.Sqft_Above+kc.Sqft_Basement)==kc.Sqft_Living).sum()) #This is the number of houses whose area above and below matches total area\n",
    "print(len(kc.Price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How wonderful! It's a perfect match, so I'll drop Sqft_Living since it's accounted for in the sum of basement and above area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop('Sqft_Living', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.drop('Sqft_Living', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll look at predictors that measure the difference between the house itself and the houses of neighbors. The interesting thing to look at here, I think, is the houses that are NOT like their neighbors. I imagine that this is where differences arise, so it might be best to create new categories for these comparators that determines if the neighbors are bigger, smaller, or the same. To do this, I'll look at the differences between the house itself and its neighbors for each of these categories.\n",
    "\n",
    "#### 3.3.1 Analyzing data of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_data = pd.DataFrame()\n",
    "neighbors_data['Lot_Difference'] = (predictors.Sqft_Lot - predictors.Sqft_Lot15)/predictors.Sqft_Lot15\n",
    "neighbors_data['Living_Difference'] = (predictors.Sqft_Above + predictors.Sqft_Basement - predictors.Sqft_Living15)/predictors.Sqft_Living15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbors_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive data alone tells us that there are outliers in the data. I'll look at histograms to better understand the data. By looking at layered slices of the data I may be able to determine where the null hypothesis $\\mu_{low}$ = $\\mu_{mid}$ = $\\mu_{high}$ can be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neighbors_data['Living_Difference']\n",
    "data.loc[(data <= data.quantile(0.0))].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def neighbor_difference(\n",
    "              data = ['Lot_Difference', 'Living_Difference'],\n",
    "              lower_cutoff = (0,0.5,0.05),\n",
    "              upper_cutoff = (0.5,1,0.05)):\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1,1,figsize = (16,9))\n",
    "    title = \"Distribution of \" + data\n",
    "    \n",
    "    data = neighbors_data[data]\n",
    "    data_low = kc.loc[data.loc[(data <= data.quantile(lower_cutoff))].index].Price.dropna()\n",
    "    data_mid = kc.loc[data.loc[(data > data.quantile(lower_cutoff)) & (data < data.quantile(upper_cutoff))].index].Price.dropna()\n",
    "    data_high = kc.loc[data.loc[(data >= data.quantile(upper_cutoff))].index].Price.dropna()\n",
    "    sns.distplot(data_low.apply(np.log10), kde = True, hist_kws=dict(alpha=0.3), ax = ax1)\n",
    "    sns.distplot(data_mid.apply(np.log10), kde = True, hist_kws=dict(alpha=0.3), ax = ax1)\n",
    "    sns.distplot(data_high.apply(np.log10), kde = True, hist_kws=dict(alpha=0.3), ax = ax1)\n",
    "    n_text = \"n Low:{0:d}\\nn Mid:{1:d}\\nn High:{2:d}\".format(len(data_low),len(data_mid),len(data_high))\n",
    "    ax1.text(-0.05, \n",
    "             0.9, \n",
    "             n_text, \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             fontsize = 16,\n",
    "             transform=ax.transAxes)\n",
    "    ax1.set_title(title)\n",
    "    plt.legend(['Low','Middle','High'], prop = {'size':14})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that a difference in lot size is a good predictor of price even at the extremes of the distributions. It makes sense not to include lot size differences in the final model.\n",
    "A difference in living area compared to the neighbors does, however, have a pretty significant effect on the price of a house. This is easily visualized by splitting the data into the bottom and top 5%. Given this finding, I'll make categories for houses in the top and bottom 5% of living area difference.\n",
    "\n",
    "Of course, if this turns out not to be a significant factor in the model then my visual analysis is incorrect. I'll assess this after the model is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neighbors_data['Living_Difference']\n",
    "kc['Living_Top'] = data >= data.quantile(0.95)\n",
    "kc.Living_Top = kc.Living_Top.astype('category')\n",
    "kc['Living_Bottom'] = data <= data.quantile(0.05)\n",
    "kc.Living_Bottom = kc.Living_Bottom.astype('category')\n",
    "kc.drop(labels = ['Sqft_Living15', 'Sqft_Lot15'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dropping Data\n",
    "Now I have to make some decisions about which numerical series to drop. I'll look at the correlations first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.corr().Price.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition, Yr_Built, and Sqft_Lot are not well correlated, so they will be dropped from the dataframe. Floors are also not well correlated and it is more highly correlated with Sqft_above, so that will be dropped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop(labels = ['Condition','Yr_Built', 'Sqft_Lot','Floors'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "corr = kc.drop(labels = 'Price', axis = 1).corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.5, vmin = -0.5, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)\n",
    "plt.show()\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but there is still correlation between predictors. I will drop more features from the dataframe if this multicollinearity becomes a problem in the model. For now, I'm content with the results and I'll move on to analyzing the effect of the zip code and month on price.\n",
    "<br>\n",
    "<br>\n",
    "### 3.5  Managing Month and Zipcode\n",
    "Zipcode is a categorical series with too many unique values to be functionally useful. RFE will take forever if there are too many dummy variables, and deciding which dummy variables to use is somewhat arbitrary and based on the total number of features. It might be best to perform target encoding with the zipcode. I'll do the same for months, though I could leave months as a categorical variable and hot-encode if the correlation is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_means = kc.groupby('Zipcode')['Price'].mean()\n",
    "month_means = kc.groupby('Month')['Price'].mean()\n",
    "kc['Zipcode_Means'] = kc.Zipcode.map(zip_means)\n",
    "kc['Month_Means'] = kc.Month.map(month_means)\n",
    "print('Zipcode Means : Price\\t' + str(kc.corr().loc['Zipcode_Means','Price']))\n",
    "print('Month : Price\\t\\t' + str(kc.corr().loc['Month_Means','Price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between Zipcode Means and Price is high, so I'll keep the target encoding and drop the Zipcodes. The correlation between Month and Price is very low, so I'll one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop('Zipcode', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In setting the dummies I found out that the dataset has duplicate entries for single ids. I'll drop the duplicates and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc = kc.reset_index().drop_duplicates(subset = 'id')\n",
    "kc['id'] = kc.id.astype('int')\n",
    "kc = kc.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dummies = pd.get_dummies(kc.Month)\n",
    "kc = kc.join(month_dummies,on = 'id')\n",
    "kc.drop(labels = ['Month', 'Month_Means'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also drop the last month, September, since it is encoded in the other month variables (if not every other month, then it must be September). This will ensure that statsmodel doesn't inject another intercept unknowingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.drop(labels = 'September', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Converting categorical data to uint8\n",
    "This is a silly step, but I have discovered that statsmodel behaves strangely when categorical data is passed to it. Statsmodel calculates an intercept when categorical data is passed into statsmodel appropriately even if the intercept has been explicitly removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = kc.select_dtypes('category')\n",
    "for column in columns:\n",
    "    kc[column] = kc[column].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Scatter Plots\n",
    "The data has been cleaned up and I now have a better sense of what to do when building the model. Now I can look at scatter plots to better understand the relatioship between the predictors and the target.\n",
    "\n",
    "One thing that I'll explore are fits for discrete data based on the median at each value. This will give me a better sense of the function that best fits each curve, which I can leverage to linearize the data before I run MLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def discrete_plot(x = ['Bedrooms', 'Bathrooms', 'Grade', 'Zipcode_Means'],\n",
    "                  y_function = ['linear', 'log'],\n",
    "                  x_function = ['linear', 'log', 'squared', 'square-root', 'exp']):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    df = pd.DataFrame(kc.groupby(x).Price.median()).reset_index()\n",
    "    x = df[x]\n",
    "    y = df.Price\n",
    "    def function_mapping(data, function):\n",
    "        function_map = {'linear':data,\n",
    "                            'log':data.apply(np.log10), \n",
    "                            'squared':data.apply(np.square),\n",
    "                            'square-root':data.apply(np.sqrt), \n",
    "                            'exp':data.apply(np.exp)}\n",
    "        return function_map[function]\n",
    "    \n",
    "    x = function_mapping(x,x_function)\n",
    "    y = function_mapping(y,y_function)\n",
    "    corr = np.corrcoef(x,y)\n",
    "    corr_text = 'corr: {:1.3f}'.format(corr[1,0])\n",
    "    sns.scatterplot(x = x, y = y)\n",
    "    plt.text(.025, \n",
    "             0.95, \n",
    "             corr_text, \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             fontsize = 16,\n",
    "             transform=ax.transAxes)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that taking the log of the price data helps linearize overall. This also helps because the distribution of the price data is normal when it is log transformed. The following relationships have the best correlation values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Relationship                 | Correlation Value |\n",
    "| ---------------------------- | :---------------: |\n",
    "| log(Price) ~ log(Bedrooms)   | 0.871             |\n",
    "| log(Price) ~ sqrt(Bathrooms) | 0.867             |\n",
    "| log(Price) ~ $Grade^2$       | 0.998             |\n",
    "| log(Price) ~ log(Zipcode)    | 0.992             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def scatter_plot(x = ['Sqft_Above','Sqft_Basement'],\n",
    "                 x_function = ['linear', 'log', 'squared', 'square-root', 'exp'],\n",
    "                 hue = ['None', 'Waterfront', 'View', 'Basement', 'Renovated']):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    def function_mapping(data, function):\n",
    "        function_map = {'linear':data,\n",
    "                            'log':data.apply(np.log10), \n",
    "                            'squared':data.apply(np.square),\n",
    "                            'square-root':data.apply(np.sqrt), \n",
    "                            'exp':data.apply(np.exp)}\n",
    "        return function_map[function]\n",
    "    x_data = kc.loc[kc[x] > 0][x]\n",
    "    x_data = function_mapping(x_data,x_function)\n",
    "    y_data = kc.loc[kc[x] > 0]['Price']\n",
    "    y_data = y_data.apply(np.log10)\n",
    "        \n",
    "    if (hue != 'None'):\n",
    "        hue = kc[hue]\n",
    "    else:\n",
    "        hue = None\n",
    "    \n",
    "    corr = np.corrcoef(x_data,y_data)\n",
    "    corr_text = 'corr: {:1.3f}'.format(corr[1,0])\n",
    "    plt.text(.025, \n",
    "             0.95, \n",
    "             corr_text, \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             fontsize = 16,\n",
    "             transform=ax.transAxes)\n",
    "    \n",
    "    sns.scatterplot(x = x_data, y = y_data, hue = hue, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not transforming the predictors yields the best correlation values, I'm concerned that there will be a non-random pattern in the residuals if they're left alone. Log transforming Sqft_Above and taking the square root of Sqft_Basement is more likely to yield a random residual pattern in both. This relationship and the correlation values is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Relationship|Correlation Value|\n",
    "|---|---|\n",
    "|log(Price) ~ log(Sqft Above)| 0.586 |\n",
    "|log(Price) ~ $SqftBasement^{1/2}$ | 0.374 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll transform the data based on the analysis that I performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kc_transformed = kc.copy()\n",
    "transform = {'Price':np.log10, \n",
    "             'Bedrooms':np.log10, \n",
    "             'Bathrooms':np.sqrt, \n",
    "             'Grade':np.square, \n",
    "             'Sqft_Above':np.log10, \n",
    "             'Sqft_Basement':np.sqrt,\n",
    "             'Zipcode_Means':np.log10}\n",
    "\n",
    "for column in list(kc.columns):\n",
    "    if column in transform.keys():\n",
    "        kc_transformed[column] = kc_transformed[column].apply(transform[column])\n",
    "kc_transformed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will normalize the data since the range for each column is significantly different from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kc_transformed_scaled = kc_transformed.copy()\n",
    "to_scale = kc_transformed_scaled.select_dtypes(include = [np.number]).columns\n",
    "for column in list(to_scale):\n",
    "    data = kc_transformed_scaled[column]\n",
    "    kc_transformed_scaled[column] = (data - data.min())/(data.max()-data.min())\n",
    "kc_transformed_scaled.agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Now I can build the model using the transformed and scaled data. I'll make training and testing sets to work with from now on. First I'll make the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = kc_transformed_scaled.drop(labels = 'Price', axis = 1).copy()\n",
    "target = kc_transformed_scaled.Price.copy()\n",
    "predictor_list = list(predictors.columns)\n",
    "formula = 'Price ~ ' + ' + '.join(list(predictors.columns)) + ' - 1'\n",
    "print(formula)\n",
    "print(len(predictors.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula accounts for all 26 predictors. I'm concerned about two things: 1) this is way too many predictors, so the model may not be sensical, and 2) there is likely still correlation between predictors. It will be easy to see if there are indeed to many predictors - the model will have some predictors whose coefficients are not statistically significant if too many predictors are confounding the results. It should also be easy to see if correlation between predictors is causing a problem by checking the values of the coefficients against our understanding of the problem. For example, we know that more bedrooms, more bathrooms, and more space indicate a higher price. If any of these coefficients are negative, then it stands to reason that interactions between the predictors is causing a problem in the model. I'll perform a multiple linear regression and look at the outputs.\n",
    "\n",
    "The train/test set is made below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictor_train, predictor_test, target_train, target_test = train_test_split(\n",
    "    predictors, target, test_size=0.20, random_state=42) # Seed with 42 for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=formula, data = predictor_train.join(target_train))\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! An $r^2$ value of 0.986! This model clearly fits the training data very well, though the same may not be the case for the test data. There are other problems, too. The fact that predictors are correlated with each other is causing problems that show up as non-sensical coefficient estimates. There is a negative correlation for bathrooms (!), for example. I'll perform RFE with cross validation in order to obtain a better model, though I imagine that I'll likely end up with the same problems since RFE only looks to optimize $r^2$, RMSE, or other similar indicators. Maybe this is an opportunity to build my own forward selector?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVR(kernel = 'linear')\n",
    "selector = RFECV(estimator = estimator, cv = 3, scoring = 'neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = selector.fit(predictor_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFE_predictors = predictor_train.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'Price ~ ' + ' + '.join(RFE_predictors) + ' - 1'\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=formula, data = predictor_train.join(target_train))\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not siginficantly better than the previous with regards to the problems identified.  There are still problems with correlation between predictors, and there are some predictors whose coefficients are not statistically significant. It's a shame that SKLearn's RFE can't catch this. I will eliminate the predictors that are insignificant and one of the offending multicollinear predictors (Bathrooms), and then rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFE_predictors = list(RFE_predictors)\n",
    "RFE_predictors.remove('Bathrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formula = 'Price ~ ' + ' + '.join(RFE_predictors) + ' - 1'\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula=formula, data = predictor_train.join(target_train))\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = res.predict(predictor_test)\n",
    "residuals = target_test - y_hat\n",
    "RMSE_test = np.sqrt((residuals**2).sum()/len(residuals))\n",
    "RMSE_train = np.sqrt(res.mse_resid)\n",
    "print('RMSE of test data:\\t' + str(RMSE_test))\n",
    "print('RMSE of train data:\\t' + str(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE of the train and test dataset are nearly identical, which tells me that the test data is well approximated by a model derived from the training data. I'll look at the residuals below to check for heteroscedasticity and non-random patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def residuals_plot(x = RFE_predictors):\n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    sns.scatterplot(x = predictor_test[x], y = residuals, ax = ax)\n",
    "    ax.set_title('Residual plot for ' + x)\n",
    "    ax.set_ylabel('Residual')\n",
    "    corr = np.corrcoef(predictor_test[x],residuals)\n",
    "    corr_text = 'corr: {:1.3f}'.format(corr[1,0])\n",
    "    plt.text(.875, \n",
    "             0.95, \n",
    "             corr_text, \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             fontsize = 16,\n",
    "             transform=ax.transAxes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals look pretty good. There is some pattern for a few of the predictors, but the correlation value does not exceed 0.254 in magnitude. I'll look at the residual distribution as well. I expect the distribution to be roughly normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.hist(bins = 20, figsize = (12,8))\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residual of transformed and scaled price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the residuals are in fact normally distributed. I'm curious about how the predictions look as compared to the actual data. I'll make a plot of that for the training set and the test set, and then I'll move on to making a function that predicts the price in dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def residual_data_plot(Show = ['Train', 'Test']):\n",
    "    df_dict = {'Train':[predictor_train, target_train],'Test':[predictor_test,target_test]}\n",
    "    df_list = df_dict[Show]\n",
    "    join = df_list[0].copy()\n",
    "    join['Price'] = df_list[1]\n",
    "    join.sort_values('Price', inplace = True)\n",
    "    y_hat = res.predict(join.drop(labels = 'Price', axis = 1))\n",
    "    MSE = np.mean(np.square(join.Price-y_hat))\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    \n",
    "    # Plot ordered y and y_hat\n",
    "    fig, ax = plt.subplots(1,1,figsize = (12,8))\n",
    "    plt.plot(np.array(y_hat), 'bo', markersize = 0.5, label = 'Predicted Price')\n",
    "    plt.plot(np.array(join.Price),'r-', label = 'Actual Price')\n",
    "    plt.legend(prop = {'size':12})\n",
    "    plt.xlabel('Houses Ordered by Price', fontdict = {'size':16})\n",
    "    plt.ylabel('Transformed and Scaled Price', fontdict = {'size':16})\n",
    "    title = 'Difference between actual and predicted price in the ' + Show.lower() + ' dataset'\n",
    "    plt.title(title, fontdict = {'size':18})\n",
    "    RMSE_text = 'RMSE: {:1.4f}'.format(RMSE)\n",
    "    plt.text(.025, \n",
    "             0.89, \n",
    "             RMSE_text, \n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             fontsize = 14,\n",
    "             transform=ax.transAxes)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(df, alpha = 0.05, kind = 'mean'):\n",
    "    # Transform the data\n",
    "    df_transformed = df.copy()\n",
    "    transform = {'Bedrooms':np.log10, \n",
    "                 'Bathrooms':np.sqrt, \n",
    "                 'Grade':np.square, \n",
    "                 'Sqft_Above':np.log10, \n",
    "                 'Sqft_Basement':np.sqrt,\n",
    "                 'Zipcode_Means':np.log10}\n",
    "\n",
    "    for column in list(df.columns):\n",
    "        if column in transform.keys():\n",
    "            df_transformed[column] = df_transformed[column].apply(transform[column])\n",
    "    \n",
    "    # Normalize the data\n",
    "    kct = kc_transformed.copy()\n",
    "    scale_factors = {'Bedrooms':{'min':kct.Bedrooms.min(), 'max':kct.Bedrooms.max()},\n",
    "                     'Bathrooms':{'min':kct.Bathrooms.min(), 'max':kct.Bathrooms.max()},\n",
    "                     'Grade':{'min':kct.Grade.min(), 'max':kct.Grade.max()},\n",
    "                     'Sqft_Above':{'min':kct.Sqft_Above.min(), 'max':kct.Sqft_Above.max()}, \n",
    "                     'Sqft_Basement':{'min':kct.Sqft_Basement.min(), 'max':kct.Sqft_Basement.max()},\n",
    "                     'Zipcode_Means':{'min':kct.Zipcode_Means.min(), 'max':kct.Zipcode_Means.max()}}\n",
    "    \n",
    "    df_transformed_scaled = df_transformed.copy()\n",
    "    to_scale = scale_factors.keys()\n",
    "    for column in list(to_scale):\n",
    "        data = df_transformed_scaled[column]\n",
    "        df_transformed_scaled[column] = (data - scale_factors[column]['min'])/(scale_factors[column]['max']-scale_factors[column]['min'])\n",
    "    \n",
    "    # Obtain the price interval\n",
    "    if kind.lower() == 'mean':\n",
    "        columns = ['mean_ci_lower', 'mean_ci_upper']\n",
    "    elif kind.lower() == 'obs':\n",
    "        columns = ['obs_ci_lower', 'obs_ci_upper']\n",
    "    else:\n",
    "        raise Exception('Kind can only be \"mean\" or \"obs\".')\n",
    "        \n",
    "    predictions = res.get_prediction(df_transformed_scaled).summary_frame(alpha = alpha)\n",
    "    predictions = predictions.loc[:,columns]\n",
    "    predictions.columns = ['Lower Price Boundary','Upper Price Boundary']\n",
    "    \n",
    "    # Denormalize the price interval (also hardcoded based on the dataset)\n",
    "    predictions = predictions*(kct.Price.max()-kct.Price.min())+kct.Price.min()\n",
    "    \n",
    "    # Transform the price interval and return\n",
    "    predictions = 10**predictions\n",
    "    predictions['id'] = df.index\n",
    "    predictions.set_index('id')\n",
    "    \n",
    "    return predictions.join(df, on = 'id').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price(kc.head(10),alpha = 0.05, kind = 'obs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `predict_price` provides a range of prices based on the input parameters. An interval for the mean price of a house with a set of attributes can be obtained by passing `kind = 'mean'`, whereas a predictive interval for a single observation can be obtained by passing `kind = 'obs'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
